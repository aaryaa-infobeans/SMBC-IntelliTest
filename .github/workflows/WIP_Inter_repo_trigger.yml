name: Workflow-Orchestrator

on:
  # repository_dispatch:
  #   types: [xray-trigger]
  # push:
  #   branches: [master, dev]
  workflow_dispatch:
    inputs:
      mode:
        description: "Execution mode (parallel/sequential)"
        required: false
        default: "sequential"
        type: choice
        options:
          - parallel-with-pytest
          - parallel-with-custom-runner
          - sequential
      workers:
        description: "Number of parallel workers (optional) (If mode is parallel-with-pytest)"
        required: false
        default: "2"
      split_level:
        description: "Split level (module/test_file) (If mode is parallel-with-pytest)"
        required: false
        default: "module"
      env:
        description: "Environment to run tests against (If mode is parallel-with-custom-runner)"
        required: true
        default: "REL"
      load_type:
        description: "Type of load to perform (If mode is parallel-with-custom-runner)"
        required: true
        default: "load"
      custom_batch_list:
        description: "List of modules to run tests against (If mode is parallel-with-custom-runner)"
        required: false
        default: "test_daily_batch"
      test_args:
        description: 'Extra pytest arguments (optional)'
        required: false
        default: ""
      xray_test_args:
        description: 'Enter Xray Test plan key or Test Execution ID (optional) in format --testplan <test_plan_key> or --execution <test_execution_id>, Also make sure the provided key should have a Jira ticket associated with it'
        required: false
        default: ""

  workflow_run:
    workflows: ["WF-Static Code Analysis"]   # Name of first workflow
    types:
      - completed

jobs:
  run_sca_first_if_manual:
    if: ${{ github.event_name == 'workflow_dispatch' }}   # Only run if triggered manually
    uses: ./.github/workflows/WF-SCA.yml   # Reuse SCA workflow
    secrets: inherit

  test-execution:
    # Ensure SCA runs and succeeds first on manual trigger; on other triggers, proceed normally
    needs: run_sca_first_if_manual
    # Run only when:
    # - Manual trigger and reusable SCA succeeded, OR
    # - Triggered by workflow_run and upstream SCA concluded success.
    # Use always() to avoid auto-skip when 'needs' is skipped on non-manual triggers.
    if: ${{ always() && ((github.event_name == 'workflow_dispatch' && needs.run_sca_first_if_manual.result == 'success') || (github.event_name == 'workflow_run' && github.event.workflow_run.conclusion == 'success')) }}
    runs-on: ubuntu-latest
    permissions:
      contents: write

    steps:
      - name: Checkout code
        uses: actions/checkout@v3

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.10'

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt
          pip install pytest-html-merger
          python -m playwright install chromium
          pip list

      - name: Check pip deps
        run: pip list
      
      - name: Prepare shell scripts
      # run only when mode is parallel-with-custom-runner
        if: ${{ github.event.inputs.mode == 'parallel-with-custom-runner' }}
        run: |
          sed -i 's/\r$//' pyRunner.sh
          chmod +x pyRunner.sh

      - name: Set env for pyRunner
        if: ${{ github.event.inputs.mode == 'parallel-with-custom-runner' }}
        run: echo "PROJECT_DIR=$GITHUB_WORKSPACE" >> $GITHUB_ENV

      - name: Set commit message and date as variables
        run: |
          echo "latestCommitMessage=$(git log -1 --pretty=format:'%s')" >> $GITHUB_ENV
          echo "currentDateTime=$(date +'%Y-%m-%d %H-%M')" >> $GITHUB_ENV
          echo "testExecutionStartedMessage=Project:SMR - Test Execution Started for RunId: ${{ github.run_id }} on ${{ env.currentDateTime }}, Branch: ${{ github.ref }}" >> $GITHUB_ENV
          echo "testExecutionStartedTitle=Project:SMR - Test Execution Started" >> $GITHUB_ENV
      
      - name: Slack Notification - Start
        uses: rtCamp/action-slack-notify@v2
        env:
          SLACK_CHANNEL: general
          SLACK_WEBHOOK: ${{ secrets.SLACK_WEBHOOK_URL }}
          SLACK_COLOR: ${{ job.status }}
          SLACK_TOKEN: ${{ secrets.SLACK_TOKEN }}
          SLACK_TITLE: ${{ env.testExecutionStartedTitle }}
          SLACK_MESSAGE: ${{ env.testExecutionStartedMessage }}
          SLACK_USERNAME: GitHub Actions
          SLACK_FOOTER: "Pytest+playwright test_automation"
        if: always()

      - name: MS Teams Message Card
        # You may pin to the exact commit or the version.
        # uses: simbo/msteams-message-card-action@d87ad6c3908b72f4fd94b55d937d05395c7300dc
        uses: simbo/msteams-message-card-action@v1.4.3
        if: always()
        with:
        # The MS Teams webhook URL to send the notification to. Obviously required.
          webhook: ${{ secrets.WEBHOOK_URL }}
            # The title of your card. Will be omitted by MS Teams if left empty.
          title: ${{ env.testExecutionStartedTitle }}
            # The message content. Supports HTML up to a certain level (interpreted by MS Teams). Can also be empty.
          message: ${{ env.testExecutionStartedMessage }}
      
      - name: Execute Tests
        continue-on-error: true
        if: ${{ github.event.inputs.mode == 'parallel-with-custom-runner' }}
        env:
          BASE_URL: 'https://www.saucedemo.com/v1/'
        run: |
          ENV_NAME="${{ github.event.inputs.env }}"
          LOAD_TYPE="${{ github.event.inputs.load_type }}"
          MODULES="${{ github.event.inputs.custom_batch_list }}"

          # Defaults for non-dispatch triggers
          if [ -z "$ENV_NAME" ]; then ENV_NAME="PROD"; fi
          if [ -z "$LOAD_TYPE" ]; then LOAD_TYPE="load"; fi

          if [ -z "$MODULES" ] || [ "$MODULES" = "all" ]; then
            python regression_runnner.py --config config.yaml --env "$ENV_NAME" --load_type "$LOAD_TYPE"
          else
            python regression_runnner.py --config config.yaml --env "$ENV_NAME" --load_type "$LOAD_TYPE" --active_batch_list "$MODULES"
          fi

      - name: Upload Test Reports
        if: ${{ github.event.inputs.mode == 'parallel-with-custom-runner' }}
        uses: actions/upload-artifact@v4
        with:
          name: pytest-reports
          path: |
            reports/**

 

      - name: Execute Tests
        id: execute_tests  # Add this ID to reference the step
        # run when mode is not parallel-with-custom-runner
        if: ${{ github.event.inputs.mode != 'parallel-with-custom-runner' }}
        continue-on-error: true
        env:
          BASE_URL: 'https://www.saucedemo.com/v1/'
          OPENAI_API_KEY: ${{ secrets.OPENAI_API_KEY }}
          XRAY_API_BASE_URL: ${{ secrets.XRAY_BASE_URL || 'https://xray.cloud.getxray.app' }}
          XRAY_CLIENT_ID: ${{ secrets.XRAY_CLIENT_ID }}
          XRAY_CLIENT_SECRET: ${{ secrets.XRAY_CLIENT_SECRET }}
          XRAY_TEST_PLAN_KEY: ${{ secrets.XRAY_TEST_PLAN_KEY }}
          XRAY_EXECUTION_SUMMARY: Smoke ${{ env.currentDateTime }}
          XRAY_EXECUTION_DESC: Test Execution after commit - ${{ env.latestCommitMessage }}

        run: |
          # Set defaults
          # Dynamically set parallel based on mode
          if [ "${{ github.event.inputs.mode }}" = "parallel-with-pytest" ]; then
            parallel="true"
          else
            parallel="false"
          fi
          
          workers="${{ github.event.inputs.workers }}"
          split="${{ github.event.inputs.split_level }}"

          # Combine additional test_args if provided
          if [ ! -z "${{ github.event.inputs.test_args }}" ]; then
            TEST_ARGS="$TEST_ARGS ${{ github.event.inputs.test_args }}"
          fi

          # Combine additional xray_test_args if provided
          if [ ! -z "${{ github.event.inputs.xray_test_args }}" ]; then
            TEST_ARGS="$TEST_ARGS ${{ github.event.inputs.xray_test_args }}"
          else
            TEST_ARGS="$TEST_ARGS --testplan ${{ secrets.XRAY_TEST_PLAN_KEY }}"
          fi

          if [ "$parallel" = "true" ]; then
            echo "➡ Running in parallel mode"

            # Fallback defaults
            if [ -z "$workers" ]; then
              workers=2
            fi
            if [ -z "$split" ]; then
              split="test_file"
            fi

            echo "Workers: $workers | Split level: $split"

            if [ "$split" = "module" ]; then
              echo "➡ Running parallaly"
              python -m pytest $TEST_ARGS --jira-xray --client-secret-auth --cloud -v -n "$workers" --dist=loadscope --junitxml=test-results.xml --json-report --json-report-file=test-results.json
            else
              python -m pytest $TEST_ARGS --jira-xray --client-secret-auth --cloud -v -n "$workers" --dist=loadfile --junitxml=test-results.xml --json-report --json-report-file=test-results.json
            fi

          else
            echo "➡ Running sequentially"
            python -m pytest $TEST_ARGS --jira-xray --client-secret-auth --cloud -v --junitxml=test-results.xml --json-report --json-report-file=test-results.json
          fi
          

          # if [ -n "${{ github.event.inputs.test_case_ids }}" ]; then
          #   test_case_ids="${{ github.event.inputs.test_case_ids }}"
          #   IFS=',' read -ra test_cases <<< "$test_case_ids"
          #   echo "Test cases requested from remote job (Jira-xray) are as follows: "
          #             # Accessing elements
          #   for tc in "${test_cases[@]}"; do
          #     echo "-> $tc"
          #   done
          #   TEST_ARGS="$TEST_ARGS --xray-ids $test_case_ids"
          # fi

          
          # # Run tests with Xray integration
          # python -m pytest \
          #   --jira-xray \
          #   --client-secret-auth \
          #   --cloud \
          #   -v \
          #   --junitxml=test-results.xml \
          #   $TEST_ARGS

      - name: Extract test statistics
        id: extract-stats
        if: always()
        run: |
          if [ -f test-results.json ]; then
            echo "passed=$(python -c "import json; data=json.load(open('test-results.json')); print(data.get('summary', {}).get('passed', 0))")" >> $GITHUB_OUTPUT
            echo "failed=$(python -c "import json; data=json.load(open('test-results.json')); print(data.get('summary', {}).get('failed', 0))")" >> $GITHUB_OUTPUT
            echo "skipped=$(python -c "import json; data=json.load(open('test-results.json')); print(data.get('summary', {}).get('skipped', 0))")" >> $GITHUB_OUTPUT
            echo "total=$(python -c "import json; data=json.load(open('test-results.json')); print(data.get('summary', {}).get('total', 0))")" >> $GITHUB_OUTPUT
          else
            echo "passed=0" >> $GITHUB_OUTPUT
            echo "failed=0" >> $GITHUB_OUTPUT
            echo "skipped=0" >> $GITHUB_OUTPUT
            echo "total=0" >> $GITHUB_OUTPUT
          fi

      - name: Set timestamp
        if: always()
        run: |
          echo "TIMESTAMP=$(date +'%Y-%m-%d_%H-%M-%S')" >> $GITHUB_ENV
          echo "TIMESTAMPFORSUBJECT=$(date +'%m-%d-%Y %H:%M:%S')" >> $GITHUB_ENV
          echo "ZipReportName=ExecutionReport_$(date +'%Y-%m-%d_%H-%M-%S').zip" >> $GITHUB_ENV
          echo "testExecutionCompletedMessage=Project:SMR - Test Execution Completed for RunId: ${{ github.run_id }} on ${{ env.currentDateTime }}, status: ${{ steps.execute_tests.outcome }} Branch: ${{ github.ref }}" >> $GITHUB_ENV
          echo "testExecutionCompletedTitle=Project:SMR - Test Execution Completed" >> $GITHUB_ENV

      - name: Generate pytest report
        if: always()
        run: python Utilities/ReportUtils/generate_pytest_report.py

      - name: Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: '18'
  
      - name: Install Allure CLI via npm
        run: |
          npm install -g allure-commandline
          allure --version
  
      - name: Generate Allure report
        run: |
          allure generate allure-results --clean -o allure-report
  
      - name: Install allure-combine and generate single file
        run: |
          pip install allure-combine
          allure-combine ./allure-report
  
      - name: Upload Allure single HTML report
        uses: actions/upload-artifact@v4
        with:
          name: allure-report-single-file
          path: SMR_allure_report.html

      - name: Upload PDF Report
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: pytest_execution_report
          path: SMR_test_summary.pdf

      - name: Upload test results to GitHub
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: test-results
          path: |
            test-results.xml
            test-results.json
            allure-results/
            SMR_allure_report.html
          retention-days: 30

      - name: Upload Allure Results
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: allure-results
          path: allure-results

      - name: Load test report history
        uses: actions/checkout@v3
        if: always()
        continue-on-error: true
        with:
          ref: gh-pages
          path: gh-pages

      - name: Build test report
        uses: simple-elf/allure-report-action@v1.7
        if: always()
        with:
          gh_pages: gh-pages
          allure_history: allure-history
          allure_results: allure-results

      - name: Publish test report
        uses: peaceiris/actions-gh-pages@v3
        if: always()
        with:
          github_token: ${{ secrets.GITHUB_TOKEN }}
          publish_branch: gh-pages
          publish_dir: allure-history
          force_orphan: true

      - name: Prepare email HTML body
        id: prepare-email
        if: always()
        run: |
          # Export environment variables for the script
          export TEST_RESULT="${{ steps.execute_tests.outcome == 'success' && 'PASSED' || 'FAILED' }}"
          export TOTAL_TESTS="${{ steps.extract-stats.outputs.total }}"
          export PASSED_TESTS="${{ steps.extract-stats.outputs.passed }}"
          export FAILED_TESTS="${{ steps.extract-stats.outputs.failed }}"
          export SKIPPED_TESTS="${{ steps.extract-stats.outputs.skipped }}"
          
          # Run the script to prepare the email body
          python .github/scripts/prepare_email_body.py
          
          # Read the generated HTML file and set it as output
          echo "email_body<<EOF" >> $GITHUB_OUTPUT
          cat email-body.html >> $GITHUB_OUTPUT
          echo "EOF" >> $GITHUB_OUTPUT

      - name: Generate test artifacts
        if: always()
        run: |
          # Create a zip file with test results
          zip -r "${{ env.ZipReportName }}" test-results.json test-results.xml || true

      - name: Send email notification
        if: always()
        uses: dawidd6/action-send-mail@v6
        with:
          server_address: smtp.gmail.com
          server_port: 587
          username: ${{ secrets.GMAIL_USERNAME }}
          password: ${{ secrets.GMAIL_APP_PASSWORD }}
          subject: "SMR Test Execution updates: ${{ steps.execute_tests.outcome == 'success' && 'PASSED' || 'FAILED' }} at ${{ env.TIMESTAMPFORSUBJECT }}"
          to: ${{ secrets.NOTIFICATION_EMAIL }}
          from: "SMR Testing Team <${{ secrets.GMAIL_USERNAME }}>"
          html_body: ${{ steps.prepare-email.outputs.email_body }}
          attachments: "${{ env.ZipReportName }},SMR_test_summary.pdf,SMR_allure_report.html"

      - name: Slack Summary - Post execution
        if: always()
        uses: rtCamp/action-slack-notify@v2
        env:
          SLACK_CHANNEL: general
          SLACK_WEBHOOK: ${{ secrets.SLACK_WEBHOOK_URL }}
          SLACK_COLOR: ${{ job.status }}
          SLACK_TOKEN: ${{ secrets.SLACK_TOKEN }}
          SLACK_TITLE: ${{ env.testExecutionCompletedTitle }}
          SLACK_MESSAGE: ${{ env.testExecutionCompletedMessage }}
          SLACK_USERNAME: GitHub Actions
          SLACK_FOOTER: "Pytest+playwright test_automation"
      
      - name: MS Teams Summary - Post execution
        if: always()
        uses: simbo/msteams-message-card-action@v1.4.3
        with:
          webhook: ${{ secrets.WEBHOOK_URL }}
          title: ${{ env.testExecutionCompletedTitle }}
          message: ${{ env.testExecutionCompletedMessage }}
